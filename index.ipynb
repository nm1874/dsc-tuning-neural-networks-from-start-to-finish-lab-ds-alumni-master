{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning and Optimizing Neural Networks - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that you've seen some regularization, initialization and optimization techniques, its time to synthesize those concepts into a cohesive modeling pipeline.  \n",
    "\n",
    "With this pipeline, you will not only fit an initial model but will also attempt to set various hyperparameters for regularization techniques. Your final model selection will pertain to the test metrics across these models. This will more naturally simulate a problem you might be faced with in practice, and the various modeling decisions you are apt to encounter along the way.  \n",
    "\n",
    "Recall that our end objective is to achieve a balance between overfitting and underfitting. You've seen the bias variance trade-off, and the role of regularization in order to reduce overfitting on training data and improving generalization to new cases. Common frameworks for such a procedure include train/validate/test methodology when data is plentiful, and K-folds cross-validation for smaller, more limited datasets. In this lab, you'll perform the latter, as the dataset in question is fairly limited. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Implement a K-folds cross validation modeling pipeline\n",
    "* Apply normalization as a preprocessing technique\n",
    "* Apply regularization techniques to improve your model's generalization\n",
    "* Choose an appropriate optimization strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Load and preview the dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 42535 entries, 0 to 42537\n",
      "Data columns (total 16 columns):\n",
      "loan_amnt              42535 non-null float64\n",
      "funded_amnt_inv        42535 non-null float64\n",
      "term                   42535 non-null object\n",
      "int_rate               42535 non-null object\n",
      "installment            42535 non-null float64\n",
      "grade                  42535 non-null object\n",
      "emp_length             41423 non-null object\n",
      "home_ownership         42535 non-null object\n",
      "annual_inc             42531 non-null float64\n",
      "verification_status    42535 non-null object\n",
      "loan_status            42535 non-null object\n",
      "purpose                42535 non-null object\n",
      "addr_state             42535 non-null object\n",
      "total_acc              42506 non-null float64\n",
      "total_pymnt            42535 non-null float64\n",
      "application_type       42535 non-null object\n",
      "dtypes: float64(6), object(10)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Your code here; load and preview the dataset\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"loan_final.csv\", header=0)\n",
    "\n",
    "# drop rows with no label\n",
    "data.dropna(subset=['total_pymnt'],inplace=True)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Problem\n",
    "\n",
    "Set up the problem by defining X and y. \n",
    "\n",
    "For this problem use the following variables for X:\n",
    "* loan_amnt\n",
    "* home_ownership\n",
    "* funded_amnt_inv\n",
    "* verification_status\n",
    "* emp_length\n",
    "* installment\n",
    "* annual_inc\n",
    "\n",
    "Our target variable y will be ```total_pymnt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here; appropriately define X and y\n",
    "import numpy as np\n",
    "\n",
    "features = ['loan_amnt', 'funded_amnt_inv', 'installment', 'annual_inc', 'home_ownership', 'verification_status', 'emp_length']\n",
    "\n",
    "X = data.loc[:, features]\n",
    "y = data.loc[:, 'total_pymnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Hold Out Test Set\n",
    "\n",
    "While we will be using K-fold cross validation to select an optimal model, we still want a final hold out test set that is completely independent of any modeling decisions. As such, pull out a sample of 10% of the total available data. For consistency of results, use random seed 123. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31901 entries, 41522 to 15725\n",
      "Data columns (total 7 columns):\n",
      "loan_amnt              31901 non-null float64\n",
      "funded_amnt_inv        31901 non-null float64\n",
      "installment            31901 non-null float64\n",
      "annual_inc             31897 non-null float64\n",
      "home_ownership         31901 non-null object\n",
      "verification_status    31901 non-null object\n",
      "emp_length             31108 non-null object\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Your code here; generate a hold out test set for final model evaluation. Use random seed 123.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps\n",
    "* Fill in missing values with SimpleImputer\n",
    "* Standardize continuous features with StandardScalar()\n",
    "* One hot encode categorical features with OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31901 entries, 0 to 31900\n",
      "Data columns (total 24 columns):\n",
      "loan_amnt                              31901 non-null float64\n",
      "funded_amnt_inv                        31901 non-null float64\n",
      "installment                            31901 non-null float64\n",
      "annual_inc                             31901 non-null float64\n",
      "home_ownership_MORTGAGE                31901 non-null float64\n",
      "home_ownership_NONE                    31901 non-null float64\n",
      "home_ownership_OTHER                   31901 non-null float64\n",
      "home_ownership_OWN                     31901 non-null float64\n",
      "home_ownership_RENT                    31901 non-null float64\n",
      "verification_status_Not Verified       31901 non-null float64\n",
      "verification_status_Source Verified    31901 non-null float64\n",
      "verification_status_Verified           31901 non-null float64\n",
      "emp_length_1 year                      31901 non-null float64\n",
      "emp_length_10+ years                   31901 non-null float64\n",
      "emp_length_2 years                     31901 non-null float64\n",
      "emp_length_3 years                     31901 non-null float64\n",
      "emp_length_4 years                     31901 non-null float64\n",
      "emp_length_5 years                     31901 non-null float64\n",
      "emp_length_6 years                     31901 non-null float64\n",
      "emp_length_7 years                     31901 non-null float64\n",
      "emp_length_8 years                     31901 non-null float64\n",
      "emp_length_9 years                     31901 non-null float64\n",
      "emp_length_< 1 year                    31901 non-null float64\n",
      "emp_length_missing                     31901 non-null float64\n",
      "dtypes: float64(24)\n",
      "memory usage: 5.8 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Select continuous features\n",
    "cont_features = ['loan_amnt', 'funded_amnt_inv', 'installment', 'annual_inc']\n",
    "X_train_cont = X_train.loc[:, cont_features]\n",
    "\n",
    "# Fill missing values with the mean\n",
    "cont_imp = SimpleImputer(strategy='mean')\n",
    "cont_imp.fit(X_train_cont)\n",
    "X_train_cont = cont_imp.transform(X_train_cont)\n",
    "\n",
    "# standardized inputs\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train_cont)\n",
    "X_train_scaled = sc.transform(X_train_cont)\n",
    "\n",
    "# Create continuous features dataframe\n",
    "cont_train_df = pd.DataFrame(X_train_scaled, columns=cont_features)\n",
    "\n",
    "# Select only the categorical features\n",
    "cat_features = ['home_ownership', 'verification_status', 'emp_length']\n",
    "X_train_cat = X_train.loc[:, cat_features]\n",
    "\n",
    "# Replace NaNs with 'missing'\n",
    "cat_imp = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "cat_imp.fit(X_train_cat)\n",
    "X_train_cat = cat_imp.transform(X_train_cat)\n",
    "\n",
    "# Encode Categorical Features\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(X_train_cat)\n",
    "X_train_ohe = ohe.transform(X_train_cat)\n",
    "\n",
    "# Create categorical features dataframe\n",
    "cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=ohe.get_feature_names(input_features=cat_features))\n",
    "\n",
    "# Combine continuous and categorical feature dataframes\n",
    "X_train_all = pd.concat([cont_train_df, cat_train_df], axis=1)\n",
    "\n",
    "X_train_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Your Holdout Set\n",
    "\n",
    "Make sure to use your StandardScalar and OneHotEncoder that you already fit on the training set to transform your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10634 entries, 0 to 10633\n",
      "Data columns (total 24 columns):\n",
      "loan_amnt                              10634 non-null float64\n",
      "funded_amnt_inv                        10634 non-null float64\n",
      "installment                            10634 non-null float64\n",
      "annual_inc                             10634 non-null float64\n",
      "home_ownership_MORTGAGE                10634 non-null float64\n",
      "home_ownership_NONE                    10634 non-null float64\n",
      "home_ownership_OTHER                   10634 non-null float64\n",
      "home_ownership_OWN                     10634 non-null float64\n",
      "home_ownership_RENT                    10634 non-null float64\n",
      "verification_status_Not Verified       10634 non-null float64\n",
      "verification_status_Source Verified    10634 non-null float64\n",
      "verification_status_Verified           10634 non-null float64\n",
      "emp_length_1 year                      10634 non-null float64\n",
      "emp_length_10+ years                   10634 non-null float64\n",
      "emp_length_2 years                     10634 non-null float64\n",
      "emp_length_3 years                     10634 non-null float64\n",
      "emp_length_4 years                     10634 non-null float64\n",
      "emp_length_5 years                     10634 non-null float64\n",
      "emp_length_6 years                     10634 non-null float64\n",
      "emp_length_7 years                     10634 non-null float64\n",
      "emp_length_8 years                     10634 non-null float64\n",
      "emp_length_9 years                     10634 non-null float64\n",
      "emp_length_< 1 year                    10634 non-null float64\n",
      "emp_length_missing                     10634 non-null float64\n",
      "dtypes: float64(24)\n",
      "memory usage: 1.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Select continuous features\n",
    "X_test_cont = X_test.loc[:, cont_features]\n",
    "\n",
    "# Fill missing values with the mean\n",
    "X_test_cont = cont_imp.transform(X_test_cont)\n",
    "\n",
    "# standardized inputs\n",
    "X_test_scaled = sc.transform(X_test_cont)\n",
    "\n",
    "# Create continuous features dataframe\n",
    "cont_test_df = pd.DataFrame(X_test_scaled, columns=cont_features)\n",
    "\n",
    "# Select only the categorical features\n",
    "X_test_cat = X_test.loc[:, cat_features]\n",
    "\n",
    "# Replace NaNs with 'missing'\n",
    "X_test_cat = cat_imp.transform(X_test_cat)\n",
    "\n",
    "# Encode Categorical Features\n",
    "X_test_ohe = ohe.transform(X_test_cat)\n",
    "\n",
    "# Create categorical features dataframe\n",
    "cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=ohe.get_feature_names(input_features=cat_features))\n",
    "\n",
    "# Combine continuous and categorical feature dataframes\n",
    "X_test_all = pd.concat([cont_test_df, cat_test_df], axis=1)\n",
    "\n",
    "X_test_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a K-fold Cross Validation Methodology\n",
    "\n",
    "Now that your have a complete holdout test set, write a function that takes in the remaining data and performs k-folds cross validation given a model object. \n",
    "\n",
    "> Note: Think about how you will analyze the output of your models in order to select an optimal model. This may involve graphs, although alternative approaches are certainly feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "\n",
    "def k_folds(features_train, labels_train, model_obj, k=10, n_epochs=100):\n",
    "    colors = sns.color_palette(\"Set2\")\n",
    "\n",
    "    validation_scores = []\n",
    "    \n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12,8))\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(features_train)):\n",
    "        \"Currently graph imaging assumes 10 folds and is hardcoded to 5x2 layout.\"\n",
    "        \n",
    "        row = i//5\n",
    "        col = i%5\n",
    "        \n",
    "        X_train, X_val = features_train.iloc[train_index], features_train.iloc[test_index]\n",
    "        y_train, y_val = labels_train.iloc[train_index], labels_train.iloc[test_index]\n",
    "        \n",
    "        model = model_obj\n",
    "        \n",
    "        hist = model.fit(X_train, y_train, batch_size=32,\n",
    "                         epochs=n_epochs, verbose=0, validation_data = (X_val, y_val))\n",
    "        #Note: verboxe=0 turns off printouts regarding training for each epoch.\n",
    "        #Potential simpler methodology\n",
    "        validation_score = model.evaluate(X_val, y_val)\n",
    "        validation_scores.append(validation_score)\n",
    "        ax = axes[row, col]\n",
    "        k = 'val_loss'\n",
    "        d = hist.history[k]\n",
    "        ax.plot(d, label=k, color=colors[0])\n",
    "\n",
    "        k = 'loss'\n",
    "        d = hist.history[k]\n",
    "        ax.plot(d, label=k, color=colors[1])\n",
    "        ax.set_title('Fold {} Validation'.format(i+1))\n",
    "        \n",
    "    #Final Graph Formatting\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\n",
    "    plt.legend(bbox_to_anchor=(1,1))\n",
    "    \n",
    "    #General Overview\n",
    "    validation_score = np.average(validation_scores)\n",
    "    print('Mean Validation Score:', validation_score)\n",
    "    print('Standard Deviation of Validation Scores:', np.std(validation_scores))\n",
    "    return validation_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Baseline Model\n",
    "\n",
    "Here, it is also important to define your evaluation metric that you will look to optimize while tuning the model. Additionally, model training to optimize this metric may consist of using a validation and test set if data is plentiful, or k-folds cross-validation if data is limited. Since this dataset is not overly large, it will be most appropriate to set up a k-folds cross-validation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here; define and compile an initial model as described\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "input_dim = X_train_all.shape[1]\n",
    "\n",
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "model.compile(optimizer=\"sgd\" ,loss='mse',metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Baseline Model with K-Folds Cross Validation\n",
    "\n",
    "Use your k-folds function to evaluate the baseline model.  \n",
    "\n",
    "Note: This code block is likely to take 10-20 minutes to run depending on the specs on your computer.\n",
    "Because of time dependencies, it can be interesting to begin timing these operations for future reference.\n",
    "\n",
    "Here's a simple little recipe to achieve this:\n",
    "```\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "later = datetime.datetime.now()\n",
    "elapsed = later - now\n",
    "print('Time Elapsed:', elapsed)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here; use your k-folds function to evaluate the baseline model.\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "k_folds(X_train_all, y_train, model)\n",
    "\n",
    "later = datetime.datetime.now()\n",
    "elapsed = later - now\n",
    "print('Time Elapsed:', elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intentionally Overfitting a Model\n",
    "\n",
    "Now that you've developed a baseline model, its time to intentionally overfit a model. To overfit a model, you can:\n",
    "* Add layers\n",
    "* Make the layers bigger\n",
    "* Increase the number of training epochs\n",
    "\n",
    "Again, be careful here. Think about the limitations of your resources, both in terms of your computers specs and how much time and patience you have to let the process run. Also keep in mind that you will then be regularizing these overfit models, meaning another round of experiments and more time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here; try some methods to overfit your network\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "input_dim = X_train_all.shape[1]\n",
    "\n",
    "#Model Mod 1: Adding More Layers\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "model.compile(optimizer=\"sgd\" ,loss='mse',metrics=['mse'])\n",
    "\n",
    "k_folds(X_train_all, y_train, model)    \n",
    "\n",
    "later = datetime.datetime.now()\n",
    "elapsed = later - now\n",
    "print('Time Elapsed:', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here; try some methods to overfit your network\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "input_dim = X_train_all.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "model.compile(optimizer=\"sgd\" ,loss='mse',metrics=['mse'])\n",
    "\n",
    "k_folds(X_train, y_train, model)    \n",
    "\n",
    "later = datetime.datetime.now()\n",
    "elapsed = later - now\n",
    "print('Time Elapsed:', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here; try some methods to overfit your network\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "input_dim = X_train_all.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "model.compile(optimizer=\"sgd\" ,loss='mse',metrics=['mse'])\n",
    "\n",
    "k_folds(X_train_all, y_train, model, n_epochs=250)\n",
    "\n",
    "later = datetime.datetime.now()\n",
    "elapsed = later - now\n",
    "print('Time Elapsed:', elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing the Model to Achieve Balance  \n",
    "\n",
    "Now that you have a powerful model (albeit an overfit one), we can now increase the generalization of the model by using some of the regularization techniques we discussed. Some options you have to try include:  \n",
    "* Adding dropout\n",
    "* Adding L1/L2 regularization\n",
    "* Altering the layer architecture (add or remove layers similar to above)  \n",
    "\n",
    "This process will be constrained by time and resources. Be sure to test at least 2 different methodologies, such as dropout and L2 regularization. If you have the time, feel free to continue experimenting.\n",
    "\n",
    "Notes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here; try some regularization or other methods to tune your network\n",
    "# L1 Regularization\n",
    "from keras import regularizers\n",
    "\n",
    "#kernel_regularizer=regularizers.l1(0.005)\n",
    "#kernel_regularizer=regularizers.l2(0.005)\n",
    "#model.add(layers.Dropout(0.3))\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "input_dim = X_train_all.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "model.compile(optimizer=\"sgd\" ,loss='mse',metrics=['mse'])\n",
    "\n",
    "k_folds(X_train_all, y_train, model, n_epochs=250) \n",
    "\n",
    "later = datetime.datetime.now()\n",
    "elapsed = later - now\n",
    "print('Time Elapsed:', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Regularization and Early Stopping\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "input_dim = X_train_all.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "model.compile(optimizer=\"sgd\" ,loss='mse',metrics=['mse'])\n",
    "\n",
    "k_folds(X_train_all, y_train, model, n_epochs=75) \n",
    "\n",
    "later = datetime.datetime.now()\n",
    "elapsed = later - now\n",
    "print('Time Elapsed:', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout and Early Stopping\n",
    "from keras import layers\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "input_dim = X_train_all.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "model.compile(optimizer=\"sgd\" ,loss='mse',metrics=['mse'])\n",
    "\n",
    "k_folds(X_train_all, y_train, model, n_epochs=75) \n",
    "\n",
    "later = datetime.datetime.now()\n",
    "elapsed = later - now\n",
    "print('Time Elapsed:', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1, Dropout and Early Stopping\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "input_dim = X_train_all.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "model.compile(optimizer=\"sgd\" ,loss='mse',metrics=['mse'])\n",
    "\n",
    "k_folds(X_train_all, y_train, model, n_epochs=75) \n",
    "\n",
    "later = datetime.datetime.now()\n",
    "elapsed = later - now\n",
    "print('Time Elapsed:', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Total Time Elapsed:', elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "Now that you have selected a network architecture, tested various regularization procedures and tuned hyperparameters via a validation methodology, it is time to evaluate your finalized model once and for all. Fit the model using all of the training and validation data using the architecture and hyperparameters that were most effective in your experiments above. Afterwards, measure the overall performance on the hold-out test data which has been left untouched (and hasn't leaked any data into the modeling process)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here; final model training on entire training set followed by evaluation on hold-out data\n",
    "\n",
    "# Based on our model runs above, it appears that using  L2 Regularization and Early Stopping\n",
    "# improves our variance 10 fold in exchange for a slight increase in MSE\n",
    "# As such, we will choose this as our final model in hopes that the model will have improved generalization\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "input_dim = X_train_all.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(10, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "model.compile(optimizer=\"sgd\" ,loss='mse',metrics=['mse'])\n",
    "\n",
    "hist =  hist = model.fit(X_train_all, y_train, batch_size=32, epochs=75)\n",
    "\n",
    "later = datetime.datetime.now()\n",
    "elapsed = later - now\n",
    "print('Time Elapsed:', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_all, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you investigated some data from *The Lending Club* in a complete data science pipeline regarding neural networks. You began with reserving a hold-out set for testing which never was touched during the modeling phase. From there, you implemented a k-fold cross validation methodology in order to assess an initial baseline model and various regularization methods. From here, you'll begin to investigate other neural network architectures such as CNNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
